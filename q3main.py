# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IE2jSevqLozjya3AsNgq6Zu1soNgUdNR

## **QUESTION 2.1**
"""


"""Importing the necessary libraries"""
import time


import os
import numpy as np
import pandas as pd
import math as m
"""Loading the dataset"""

dataset_dir = os.path.dirname(os.path.abspath(__file__))

csv_path = os.path.join(dataset_dir, 'y_train.csv')

column_names = ["label"]

df = pd.read_csv(csv_path, names = column_names, header=None)
df = df[1:]
df

df_arr = np.array(df)
spams = np.where(df_arr == '1')
len(spams[0])

"""**Q.2.1.1**
The number of mails with spam: 1183
The number of all mails: 4137
The probability of spam mails: 0.2860
"""

num = '1'
print("The number of mails with spam: " + str(len(df[df["label"] == num])))
print("The number of all mails: " + str(len(df)))
print(len(df[df["label"] == num])/len(df))

"""**Q.2.1.2**
We have 1183 spam mails and we have 2954 ham mails which means that the probability of spam mails is 0.286 and the probability of ham mails is 0.714. These results affect the training model. The minority class is spam and due to being between 20-40% of the all dataset, the dataset has mild imbalance problem. Because of this issue, the training model spend its most of the time on ham examples and it cannot learn enough from spam mails. It is not enough informative for training model

**Q.2.1.3** In this imbalanced dataset, accuracy will not show reliable results and we will face with the limitation of accuracy because in this dataset, the training model always predicts the ham mails and it will perform poorly in detecting the spam mails. However, because of detecting the ham mails correctly and the majority of the dataset is formed by ham mails, accuracy will be high although the training model predicts poorly the spam dataset. Hence, we will need other metrics to see the success of training model better.
"""


"""**Q.2.2** In the below code, I extracted an added the frequencies of the features, then divide to the total words. In this part I specified the a=0 because we dont do any additive smoothing for this part. In the last section, accuracy and confusion matrix results are printed."""

def load_dataset(csv_path):
  df = pd.read_csv(csv_path)
  return df

#creating dataframes of train csv files


def extract_mails(concat_df, isSpam):
  spam_df = concat_df[concat_df['Prediction'] == isSpam]
  number_of_mail = spam_df.shape[0]
  spam_df = spam_df.drop('Prediction', axis=1)

  #summing the features
  feature_sums = spam_df.sum(axis=0)

  #summing all words
  total_words = spam_df.sum().sum()

  return feature_sums, total_words, number_of_mail


def find_probs_features(a):  #a for when a=0 and a=5 (smoothing)
  prob_of_spam = (features_spam + a) /(total_words_spam + a * length)
  prob_of_normal = (features_normal + a) /(total_words_normal + a * length)
  return prob_of_spam,prob_of_normal

prob_of_spam = 0
prob_of_normal =0
number_of_spam_mail = 0
number_of_normal_mail = 0
number_of_total_mail = 0

def evaluate_on_test_set(test_x, isSpam):

  l = []
  if isSpam :
    prob_res = prob_of_spam
    prob_of_mail = number_of_spam_mail / number_of_total_mail
  else:
    prob_res = prob_of_normal
    prob_of_mail = number_of_normal_mail / number_of_total_mail

  for row in test_x.itertuples(index=False):
    sum = 0
    for column_index, cell in enumerate(row):
      if cell != 0:
        if prob_res[column_index] == 0:
          sum = sum -(10**12)
        else:
            sum = sum + cell * np.log(prob_res[column_index], where=prob_res[column_index]>0) 
    sum = sum + np.log(prob_of_mail, where= prob_of_mail>0) 
    l.append(float(sum))

  res_test_x = pd.DataFrame(l)

  return res_test_x

  

def compare_classes(spam,normal):
  col_name ="Prediction"
  label_set = pd.DataFrame(np.maximum(spam.values, normal.values) == spam.values, columns=[col_name])
  return label_set.astype(int)



def find_res_for_a(a):
  global prob_of_spam
  global prob_of_normal
  global number_of_spam_mail
  global number_of_normal_mail
  global number_of_total_mail
  global features_spam, total_words_spam,total_words_normal,features_normal,length

  csv_path = os.path.join(dataset_dir, 'x_train.csv')
  csv_path2 = os.path.join(dataset_dir, 'y_train.csv')
  train_x = load_dataset(csv_path)
  train_y = load_dataset(csv_path2)

  concat_df = pd.concat([train_x, train_y], axis=1)
  features_spam, total_words_spam,number_of_spam_mail = extract_mails(concat_df,1)
  features_normal, total_words_normal, number_of_normal_mail = extract_mails(concat_df, 0)
  number_of_total_mail = train_x.shape[0]
  print("Total number of mails:" + str(number_of_total_mail))
  length = features_spam.shape[0]

  prob_of_spam,prob_of_normal=find_probs_features(a) #WE WILL CHANGE THE A VALUE FOR SMOOTHING THE TRAINING
  #Extracting test datasets
  csv_path = os.path.join(dataset_dir, 'x_test.csv')
  csv_path2 = os.path.join(dataset_dir, 'y_test.csv')
  test_x =load_dataset(csv_path)
  test_y = load_dataset(csv_path2)

  res_x_spam = evaluate_on_test_set(test_x, 1)
  res_x_normal = evaluate_on_test_set(test_x, 0)
  sets_with_labels = compare_classes(res_x_spam,res_x_normal)
  compare_results = (sets_with_labels == test_y)

  #Printing the accuracy of the result
  correct_pred = len(compare_results[compare_results["Prediction"] == True])
  print(len(compare_results))
  accuracy = correct_pred / len(compare_results)
  print("Accuracy: " + str(accuracy))

  #Printing the confusion matrix of the results
  num_classes = 2 #for 1 and 0
  conf_matrix = [[0] * num_classes for _ in range(num_classes)]
  print(len(test_y))
  for index, row in test_y.iterrows():
    pred_result = sets_with_labels.iloc[index,0]
    actual_result = row[0]
    index_r = 1-pred_result
    index_c = 1- actual_result

    conf_matrix[int(index_r)][int(index_c)] += 1

  print(conf_matrix)
  print("Number of true positives: " + str(conf_matrix[0][0]))
  print("Number of false positives: " + str(conf_matrix[0][1]))
  print("Number of true negatives: " + str(conf_matrix[1][1]))
  print("Number of false negatives: " + str(conf_matrix[1][0]))

  print("Number of false predictions: " + str(conf_matrix[0][1]+conf_matrix[1][0]))

print(10*"*" + "MULTINOMIAL NAIVE BAYES FOR A=0"+ 10*"*")
find_res_for_a(0)
print(10*"*" + "MULTINOMIAL NAIVE BAYES FOR A=5"+ 10*"*")
find_res_for_a(5)
"""**Q.2.3** In this part I aonly changed a to 5 to and then I automatically added the smoothing addtivie to the results. The results can be the seen in the above output.

**Q.2.4.**
In the below part, I will build a bernoulli naive bayes training model
"""
start = time.time()


def extract_mail_bernoulli(concat_df, isSpam):
  spam_df = concat_df[concat_df['Prediction'] == isSpam]
  number_of_mail = spam_df.shape[0]
  spam_df = spam_df.drop('Prediction', axis=1)

  #summing the features
  feature_sums = spam_df.sum(axis=0)

  return feature_sums, number_of_mail

csv_path = os.path.join(dataset_dir, 'x_train.csv')
csv_path2 = os.path.join(dataset_dir, 'y_train.csv')
train_x = load_dataset(csv_path)
train_y = load_dataset(csv_path2)
length = train_x.shape[1]
bin_train_x = train_x.applymap(lambda x: 1 if x > 0 else 0)
concat_df = pd.concat([bin_train_x, train_y], axis=1)
features_spam_b, number_of_spam_mail_b = extract_mail_bernoulli(concat_df,1)
features_normal_b, number_of_normal_mail_b = extract_mail_bernoulli(concat_df, 0)
length_b = features_spam_b.shape[0]
prob_of_spam_b = (features_spam_b) /(number_of_spam_mail_b)
prob_of_normal_b = (features_normal_b ) /(number_of_normal_mail_b)


def evaluate_on_test_set_bernoulli(test_x, isSpam):
  

  #res_test_x = test_x.apply(evaluate_value_bernoulli, axis=1, args=isSpam)

  if isSpam :
    prob_res = prob_of_spam_b
    prob_of_mail = number_of_spam_mail_b / number_of_total_mail
  else:
    prob_res = prob_of_normal_b
    prob_of_mail = number_of_normal_mail_b / number_of_total_mail
  l = []
  for row in test_x.itertuples(index=False):
    sum = 0
    for column_index, cell in enumerate(row):
      if cell != 0:
        if prob_res[column_index] ==0:
          sum = sum -(10**12)
        else:
          sum = sum + m.log(prob_res[column_index])
      else:
        if 1-prob_res[column_index] != 0:
          sum = sum +  m.log(1-prob_res[column_index])
        else:
          sum = sum -(10**12)        

    sum = sum + m.log(prob_of_mail) if m.log(prob_of_mail) != 0 else sum -(10**12)      
    l.append(float(sum))
  

  res_test_x = pd.DataFrame(l)

  return res_test_x

csv_path = os.path.join(dataset_dir, 'x_test.csv')
csv_path2 = os.path.join(dataset_dir, 'y_test.csv')
test_x =load_dataset(csv_path)
test_y = load_dataset(csv_path2)


res_x_spam = evaluate_on_test_set_bernoulli(test_x, 1)
res_x_normal = evaluate_on_test_set_bernoulli(test_x, 0)
sets_with_labels = compare_classes(res_x_spam,res_x_normal)

compare_results = (sets_with_labels == test_y)

#Printing the accuracy of the result
print(10*"*" + "BERNOULLÄ° NAIVE BAYES"+ 10*"*")
correct_pred = len(compare_results[compare_results["Prediction"] == True])
accuracy = correct_pred / len(compare_results)
print("Accuracy: " + str(accuracy))

#Printing the confusion matrix of the results
num_classes = 2 #for 1 and 0
conf_matrix = [[0] * num_classes for _ in range(num_classes)]
print(len(test_y))
for index, row in test_y.iterrows():
  pred_result = sets_with_labels.iloc[index,0]
  actual_result = row[0]
  index_r = 1-pred_result
  index_c = 1- actual_result

  conf_matrix[int(index_r)][int(index_c)] += 1

print(conf_matrix)
print("Number of true positives: " + str(conf_matrix[0][0]))
print("Number of false positives: " + str(conf_matrix[0][1]))
print("Number of true negatives: " + str(conf_matrix[1][1]))
print("Number of false negatives: " + str(conf_matrix[1][0]))

print("Number of false predictions: " + str(conf_matrix[0][1]+conf_matrix[1][0]))

end = time.time()
print("Execution Time:" + str(end-start))